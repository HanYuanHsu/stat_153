---
title: "hw4code"
author: "Han-Yuan Hsu"
date: '2022-10-26'
output: pdf_document
---
```{r}
library(dplyr)
library(ggplot2)
set.seed(111)
```
Reference: lecture code

# 1
Consider the dataset lynx that is available in base R. This gives the annual numbers
of lynx trappings for 1821-1934 in Canada. Type help(lynx) to learn more about the
dataset.
```{r}
data("lynx")
n = length(lynx) # 114
```

```{r}
plot(1:90, lynx[1:90], type='l')
```



## a
Fit the AR(2) model to the first 90 observations of this dataset. Report the
estimates of $\phi_0, \phi_1, \phi_2, \sigma$ along with uncertainty quantification. (3 points)
```{r}
p = 2
Xmat = matrix(1, (90-p), 1)
for(j in 1:p)
{
   Xmat = cbind(Xmat, lynx[(p+1-j):(90-j)])
}
modar = lm(lynx[(p+1):90] ~ -1 + Xmat)
summary(modar)
```

The point estimates for $\phi_0, \phi_1, \phi_2$ are
```{r}
modar$coefficients
```
The point estimate for $\sigma$ is
```{r}
sighat = sqrt((sum((modar$residuals)^2))/85) # 85=90-2*2-1
sighat
```
Why is sighat so large. Possible reason: see the plot below. The AR(2) model does not expect lynx data to have big
peaks every 40 years

Uncertainty quantification: \\
The posterior phi's follow a multivariate t distribution.
```{r}
cov.mat = sighat^2 * solve(t(Xmat) %*% Xmat)
sd = sqrt(diag(cov.mat))
sd
```
The rows below are 95% confidence intervals for $\phi_0, \phi_1, \phi_2$:
```{r}
for(i in 1:3) {
  a = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.025, df=90-2*2-1))
  b = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.975, df=90-2*2-1))
  print(c(a,b))
}
```

From homework 1, we know the posterior $\sigma$ is such that
$$\frac{\text{RSS}}{\sigma^2}$$
follows the chi-square distribution with 90-2*2-1 degrees of freedom. Below is a 95% CI for $\sigma$:
```{r}
chi975 = qchisq(.975, df=90-5)
chi025 = qchisq(.025, df=90-5)
RSS = sum((modar$residuals)^2)
c(sqrt(RSS/chi975), sqrt(RSS/chi025))
```

\newpage
## b
Write down an explicit formula for the predictions generated by your fitted AR(2)
model for Yt for t $\geq$ 91. (4 points)

The explicit formula is of the form `c + r^t * (a*cos(theta*t) + b*sin(theta*t))`, where c, r, a, b, and theta will be calculated in the following code blocks.

```{r}
phi0 = modar$coefficients[1]
phi1 = modar$coefficients[2]
phi2 = modar$coefficients[3]
root = polyroot(c(-phi2, -phi1, 1))[1]
r = Mod(root)
theta = Arg(root)
r
theta
```
```{r}
c = as.numeric(phi0 / (1-phi1-phi2))
c
```
```{r}
a = lynx[89] - c
b = ((lynx[90]-c)/r - a*cos(theta))/sin(theta)
a
b
```
Below is the explicit formula for the predicted $y_{t}$, where $t \geq 91$:
```{r}
pred = function(t) {
  t = t-89
  return( c + r^t * (a*cos(theta*t) + b*sin(theta*t)) )
}
```


## c
Use your AR(2) to predict the data from time points t = 91, . . . , 114. Also
compute the standard deviations corresponding to the accuracy of prediction. (4
points).
```{r}
y.pred = rep(-9999, n)
y.pred[1:90] = lynx[1:90]
for (k in 91:n) {
  y.pred[k] = sum(modar$coefficients * c(1, y.pred[k-1], y.pred[k-2]))
}

plot(1:n, y.pred, type='l')
abline(v=90, col='red')
#lines(89:n, sapply(89:n, pred), col='blue')
```

Calculate covariance matrix of prediction variables, Gamhat:
```{r}
Gamhat = matrix(sighat^2, 1, 1) #this is the uncertainty for the first i.e., (n+1)^th prediction
vkp = matrix(modar$coefficients[2], 1, 1) #this is the estimate for phi1
for(i in 1:(n-90-1))
{
    covterm = Gamhat %*% vkp
    varterm = (sighat^2) + (t(vkp) %*% (Gamhat %*% vkp))
    Gamhat = cbind(Gamhat, covterm)
    Gamhat = rbind(Gamhat, c(t(covterm), varterm))
    
    if (i < p) {vkp = c(modar$coefficients[(i+2)], vkp)}
    if (i >= p) {vkp = c(0, vkp)}
}
```
The standard deviations corresponding to the accuracy of prediction:
```{r}
predsd = sqrt(diag(Gamhat))
predsd
```

## d
Compare your predictions with the actual values from the dataset. Comment on
the accuracy of the predictions. (2 points)

In the plot below, the black graph is the actual lynx data, the red graphs are the predictions plus the 2-sd bands.
```{r}
plot(1:n, lynx, type='l', ylim=c(-2000, 7000))
lines(91:n, y.pred[91:n], col='red')
predlower = y.pred[91:n] - 2*predsd
predupper = y.pred[91:n] + 2*predsd
lines(91:n, predlower, col='red')
lines(91:n, predupper, col='red')
```
Comment on
the accuracy of the predictions.................................

\newpage
# 2
Consider the US population dataset from https://fred.stlouisfed.org/series/
POPTHM that we have worked with in class.
```{r}
pop = read.csv('POPTHM.csv')
y = pop$POPTHM
n = length(y)
```
```{r}
plot(1:n, y, type='l')
```



## a
Fit an AR(2) model to this dataset. Report the estimates of the parameters along
with uncertainty quantification (3 points).
```{r}
Xmat = matrix(1, nrow=n-2, ncol=3)
Xmat[,2] = y[2:(n-1)]
Xmat[,3] = y[1:(n-2)]
modar = lm(y[3:n] ~ -1 + Xmat)
summary(modar)
```

The point estimates for $\phi_0, \phi_1, \phi_2$ are
```{r}
modar$coefficients
```
The point estimate for $\sigma$ is
```{r}
sighat = sqrt((sum((modar$residuals)^2))/(n-2*2-1)) 
sighat
```

Uncertainty quantification: \\
The posterior phi's follow a multivariate t distribution.
```{r}
cov.mat = sighat^2 * solve(t(Xmat) %*% Xmat)
sd = sqrt(diag(cov.mat))
sd
```
The rows below are 95% confidence intervals for $\phi_0, \phi_1, \phi_2$:
```{r}
for(i in 1:3) {
  a = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.025, df=n-2*2-1))
  b = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.975, df=n-2*2-1))
  print(c(a,b))
}
```
Below is a 95% CI for $\sigma$:
```{r}
chi975 = qchisq(.975, df=n-5)
chi025 = qchisq(.025, df=n-5)
RSS = sum((modar$residuals)^2)
c(sqrt(RSS/chi975), sqrt(RSS/chi025))
```

## b
Write down an explicit formula for the predictions generated by your fitted AR(2)
model for Yt for the future months. (4 points)


```{r}
phi0 = modar$coefficients[1]
phi1 = modar$coefficients[2]
phi2 = modar$coefficients[3]
roots = polyroot(c(-phi2, -phi1, 1)) # real roots
roots
alpha = Re(roots[1]) # convert to real numbers
beta = Re(roots[2])
```
```{r}
c = as.numeric(phi0 / (1-phi1-phi2))
c
```
```{r}
a = (beta*(y[n-1]-c) - (y[n]-c)) / (beta-alpha)
b = (-alpha*(y[n-1]-c) + (y[n]-c)) / (beta-alpha)
a
b
```
Below is the explicit formula for the predicted $y_{t}$, where $t > n$:
```{r}
pred <- function(t) {
  t = t-(n-1)
  c + a*alpha^t + b*beta^t
}
```


## c
Use your AR(2) model to predict the data for 36 months immediately succeeding
the last month in the dataset. Plot these predictions and uncertainty indicators
along with the original data. Do these predictions make intuitive sense? (6 points)

The prediction starts after the red line; the part on the left side of the red line is the original data.
```{r}
y.pred = c(y, sapply((n+1):(n+36), pred))
plot(1:(n+36), y.pred, type='l')
abline(v=n, col='red')
#lines(89:n, sapply(89:n, pred), col='blue')
```
Calculate covariance matrix of prediction variables, Gamhat:
```{r}
Gamhat = matrix(sighat^2, 1, 1) #this is the uncertainty for the first i.e., (n+1)^th prediction
vkp = matrix(modar$coefficients[2], 1, 1) #this is the estimate for phi1
for(i in 1:(36-1))
{
    covterm = Gamhat %*% vkp
    varterm = (sighat^2) + (t(vkp) %*% (Gamhat %*% vkp))
    Gamhat = cbind(Gamhat, covterm)
    Gamhat = rbind(Gamhat, c(t(covterm), varterm))
    
    # update vkp
    if (i < p) {vkp = c(modar$coefficients[(i+2)], vkp)}
    if (i >= p) {vkp = c(0, vkp)}
}
```

The standard deviations corresponding to the accuracy of prediction:
```{r}
predsd = sqrt(diag(Gamhat))
predsd
```

Below is the plot of the predictions along with some original data right before the prediction starts. The shaded area represents the uncertainty of the predictions.
```{r}
pred.time = (n+1):(n+36)
pred.part = sapply(pred.time, pred)
predupper = pred.part + 2*predsd
predlower = pred.part - 2*predsd
yupper = c(y, predupper)
ylower = c(y, predlower)

start = 651 # we only plot observations after this time (inclusive)
df = data.frame(
  t = start:(n+36),
  y = y.pred[start:(n+36)],
  yupper = yupper[start:(n+36)],
  ylower = ylower[start:(n+36)]
)

ggplot(df) + 
  geom_line(aes(x=t, y=y)) +
  geom_vline(xintercept = n, color='red') + 
  geom_ribbon(aes(x=t, ymin=ylower, ymax=yupper), alpha=.2)
```

The predictions make sense: because the largest root $\beta$ is very close to 1, the prediction graph is almost linear, and the slope looks similar to the slope of the data. 

## d
Suppose that we want to predict the US population for the months preceding
January 1959. Compare your fitted backward model with the forward model fitted earlier. Are there any
similarities between the two models? (4 points)

## e
Using your model from the previous part, predict the US population for the 36
months immediately preceding January 1959. Plot these predictions and uncertainty indicators along with the original data. Do these predictions make intuitive sense? (6 points).

