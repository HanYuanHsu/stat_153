---
title: "hw4code"
author: "Han-Yuan Hsu"
date: '2022-10-26'
output: pdf_document
---
```{r}
library(tidyverse)
set.seed(111)
```
Reference: lecture code

# 1
Consider the dataset lynx that is available in base R. This gives the annual numbers
of lynx trappings for 1821-1934 in Canada. Type help(lynx) to learn more about the
dataset.
```{r}
data("lynx")
n = length(lynx) # 114
```

```{r}
plot(1:90, lynx[1:90], type='l')
```



## a
Fit the AR(2) model to the first 90 observations of this dataset. Report the
estimates of $\phi_0, \phi_1, \phi_2, \sigma$ along with uncertainty quantification. (3 points)
```{r}
p = 2
Xmat = matrix(1, (90-p), 1)
for(j in 1:p)
{
   Xmat = cbind(Xmat, lynx[(p+1-j):(90-j)])
}
modar = lm(lynx[(p+1):90] ~ -1 + Xmat)
summary(modar)
```

The point estimates for $\phi_0, \phi_1, \phi_2$ are
```{r}
modar$coefficients
```
The point estimate for $\sigma$ is
```{r}
sighat = sqrt((sum((modar$residuals)^2))/85) # 85=90-2*2-1
sighat
```
Why is sighat so large. Possible reason: see the plot below. The AR(2) model does not expect lynx data to have big
peaks every 40 years

Uncertainty quantification: \\
The posterior phi's follow a multivariate t distribution.
```{r}
cov.mat = sighat^2 * solve(t(Xmat) %*% Xmat)
sd = sqrt(diag(cov.mat))
sd
```
The rows below are 95% confidence intervals for $\phi_0, \phi_1, \phi_2$:
```{r}
for(i in 1:3) {
  a = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.025, df=90-2*2-1))
  b = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.975, df=90-2*2-1))
  print(c(a,b))
}
```

From homework 1, we know the posterior $\sigma$ is such that
$$\frac{\text{RSS}}{\sigma^2}$$
follows the chi-square distribution with 90-2*2-1 degrees of freedom. Below is a 95% CI for $\sigma$:
```{r}
chi975 = qchisq(.975, df=90-5)
chi025 = qchisq(.025, df=90-5)
RSS = sum((modar$residuals)^2)
c(sqrt(RSS/chi975), sqrt(RSS/chi025))
```

\newpage
## b
Write down an explicit formula for the predictions generated by your fitted AR(2)
model for Yt for t $\geq$ 91. (4 points)

The explicit formula is of the form `c + r^t * (a*cos(theta*t) + b*sin(theta*t))`, where c, r, a, b, and theta will be calculated in the following code blocks.

```{r}
phi0 = modar$coefficients[1]
phi1 = modar$coefficients[2]
phi2 = modar$coefficients[3]
root = polyroot(c(-phi2, -phi1, 1))[1]
r = Mod(root)
theta = Arg(root)
r
theta
```
```{r}
c = as.numeric(phi0 / (1-phi1-phi2))
c
```
```{r}
a = lynx[89] - c
b = ((lynx[90]-c)/r - a*cos(theta))/sin(theta)
a
b
```
Below is the explicit formula for the predicted $y_{t}$, where $t \geq 91$:
```{r}
pred = function(t) {
  t = t-89
  return( c + r^t * (a*cos(theta*t) + b*sin(theta*t)) )
}
```


## c
Use your AR(2) to predict the data from time points t = 91, . . . , 114. Also
compute the standard deviations corresponding to the accuracy of prediction. (4
points).
```{r}
y.pred = rep(-9999, n)
y.pred[1:90] = lynx[1:90]
for (k in 91:n) {
  y.pred[k] = sum(modar$coefficients * c(1, y.pred[k-1], y.pred[k-2]))
}

plot(1:n, y.pred, type='l')
abline(v=90, col='red')
#lines(89:n, sapply(89:n, pred), col='blue')
```

Calculate covariance matrix of prediction variables, Gamhat:
```{r}
Gamhat = matrix(sighat^2, 1, 1) #this is the uncertainty for the first i.e., (n+1)^th prediction
vkp = matrix(modar$coefficients[2], 1, 1) #this is the estimate for phi1
for(i in 1:(n-90-1))
{
    covterm = Gamhat %*% vkp
    varterm = (sighat^2) + (t(vkp) %*% (Gamhat %*% vkp))
    Gamhat = cbind(Gamhat, covterm)
    Gamhat = rbind(Gamhat, c(t(covterm), varterm))
    
    if (i < p) {vkp = c(modar$coefficients[(i+2)], vkp)}
    if (i >= p) {vkp = c(0, vkp)}
}
```
The standard deviations corresponding to the accuracy of prediction:
```{r}
predsd = sqrt(diag(Gamhat))
predsd
```

## d
Compare your predictions with the actual values from the dataset. Comment on
the accuracy of the predictions. (2 points)

In the plot below, the black graph is the actual lynx data, the red graphs are the predictions plus the 2-sd bands.
```{r}
plot(1:n, lynx, type='l', ylim=c(-2000, 7000))
lines(91:n, y.pred[91:n], col='red')
predlower = y.pred[91:n] - 2*predsd
predupper = y.pred[91:n] + 2*predsd
lines(91:n, predlower, col='red')
lines(91:n, predupper, col='red')
```
Comment on
the accuracy of the predictions.................................

\newpage
# 2
Consider the US population dataset from https://fred.stlouisfed.org/series/
POPTHM that we have worked with in class.
```{r}
pop = read.csv('POPTHM.csv')
y = pop$POPTHM
n = length(y)
```
```{r}
plot(1:n, y, type='l')
```



## a
Fit an AR(2) model to this dataset. Report the estimates of the parameters along
with uncertainty quantification (3 points).
```{r}
Xmat = matrix(1, nrow=n-2, ncol=3)
Xmat[,2] = y[2:(n-1)]
Xmat[,3] = y[1:(n-2)]
modar = lm(y[3:n] ~ -1 + Xmat)
summary(modar)
```

The point estimates for $\phi_0, \phi_1, \phi_2$ are
```{r}
modar$coefficients
```
The point estimate for $\sigma$ is
```{r}
sighat = sqrt((sum((modar$residuals)^2))/(n-2*2-1)) 
sighat
```

Uncertainty quantification: \\
The posterior phi's follow a multivariate t distribution.
```{r}
cov.mat = sighat^2 * solve(t(Xmat) %*% Xmat)
sd = sqrt(diag(cov.mat))
sd
```
The rows below are 95% confidence intervals for $\phi_0, \phi_1, \phi_2$:
```{r}
for(i in 1:3) {
  a = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.025, df=n-2*2-1))
  b = as.numeric(modar$coefficients[i] + sd[i]*qt(p=.975, df=n-2*2-1))
  print(c(a,b))
}
```
Below is a 95% CI for $\sigma$:
```{r}
chi975 = qchisq(.975, df=n-5)
chi025 = qchisq(.025, df=n-5)
RSS = sum((modar$residuals)^2)
c(sqrt(RSS/chi975), sqrt(RSS/chi025))
```

## b
Write down an explicit formula for the predictions generated by your fitted AR(2)
model for Yt for the future months. (4 points)


```{r}
phi0 = modar$coefficients[1]
phi1 = modar$coefficients[2]
phi2 = modar$coefficients[3]
roots = polyroot(c(-phi2, -phi1, 1)) # real roots
roots
alpha = Re(roots[1]) # convert to real numbers
beta = Re(roots[2])
```
```{r}
c = as.numeric(phi0 / (1-phi1-phi2))
c
```
```{r}
a = (beta*(y[n-1]-c) - (y[n]-c)) / (beta-alpha)
b = (-alpha*(y[n-1]-c) + (y[n]-c)) / (beta-alpha)
a
b
```
Below is the explicit formula for the predicted $y_{t}$, where $t > n$:
```{r}
pred <- function(t) {
  t = t-(n-1)
  c + a*alpha^t + b*beta^t
}
```


## c
Use your AR(2) model to predict the data for 36 months immediately succeeding
the last month in the dataset. Plot these predictions and uncertainty indicators
along with the original data. Do these predictions make intuitive sense? (6 points)

The prediction starts after the red line; the part on the left side of the red line is the original data.
```{r}
y.pred = c(y, sapply((n+1):(n+36), pred))
plot(1:(n+36), y.pred, type='l')
abline(v=n, col='red')
#lines(89:n, sapply(89:n, pred), col='blue')
```
Calculate covariance matrix of prediction variables, Gamhat:
```{r}
Gamhat = matrix(sighat^2, 1, 1) #this is the uncertainty for the first i.e., (n+1)^th prediction
vkp = matrix(modar$coefficients[2], 1, 1) #this is the estimate for phi1
for(i in 1:(36-1))
{
    covterm = Gamhat %*% vkp
    varterm = (sighat^2) + (t(vkp) %*% (Gamhat %*% vkp))
    Gamhat = cbind(Gamhat, covterm)
    Gamhat = rbind(Gamhat, c(t(covterm), varterm))
    
    # update vkp
    if (i < p) {vkp = c(modar$coefficients[(i+2)], vkp)}
    if (i >= p) {vkp = c(0, vkp)}
}
```

The standard deviations corresponding to the accuracy of prediction:
```{r}
predsd = sqrt(diag(Gamhat))
predsd
```

Below is the plot of the predictions along with some original data right before the prediction starts. The shaded area represents the uncertainty of the predictions.
```{r}
pred.time = (n+1):(n+36)
pred.part = sapply(pred.time, pred)
predupper = pred.part + 2*predsd
predlower = pred.part - 2*predsd
yupper = c(y, predupper)
ylower = c(y, predlower)

start = 651 # we only plot observations after this time (inclusive)
df = data.frame(
  t = start:(n+36),
  y = y.pred[start:(n+36)],
  yupper = yupper[start:(n+36)],
  ylower = ylower[start:(n+36)]
)

ggplot(df) + 
  geom_line(aes(x=t, y=y)) +
  geom_vline(xintercept = n, color='red') + 
  geom_ribbon(aes(x=t, ymin=ylower, ymax=yupper), alpha=.2)
```

The predictions make sense: because the largest root $\beta$ is very close to 1, the prediction graph is almost linear, and the slope looks similar to the slope of the data. Also, the uncertainty interval is wider for predictions that are farther in the future.

## d
Suppose that we want to predict the US population for the months preceding
January 1959. Compare your fitted backward model with the forward model fitted earlier. Are there any
similarities between the two models? (4 points)
```{r}
Xmat = matrix(1, nrow=n-2, ncol=3)
Xmat[,2] = y[2:(n-1)]
Xmat[,3] = y[3:n]
modar.back = lm(y[1:(n-2)] ~ -1 + Xmat)
summary(modar.back)
```

```{r}
modar$coefficients
```
```{r}
modar.back$coefficients
```



```{r}
alph0 = modar.back$coefficients[1]
alph1 = modar.back$coefficients[2]
alph2 = modar.back$coefficients[3]

-alph0/alph2
-alph1/alph2
1/alph2
```
WEIRD????????????????/

## e
Using your model from the previous part, predict the US population for the 36
months immediately preceding January 1959. Plot these predictions and uncertainty indicators along with the original data. Do these predictions make intuitive sense? (6 points).

```{r}
y.full = c(rep(-9999,36), y) # y prepended with the 36 backward predictions
k = 36
while(k>0) {
  y.full[k] = sum(modar.back$coefficients * c(1, y.pred[k+1], y.pred[k+2]))
  k = k-1
}

sighat = sqrt((sum((modar.back$residuals)^2))/(n-2*2-1)) #note: we use modar.back here
Gamhat = matrix(sighat^2, 1, 1)
vkp = matrix(modar.back$coefficients[2], 1, 1) #note: we use modar.back here
for(i in 1:(36-1))
{
    covterm = Gamhat %*% vkp
    varterm = (sighat^2) + (t(vkp) %*% (Gamhat %*% vkp))
    Gamhat = cbind(Gamhat, covterm)
    Gamhat = rbind(Gamhat, c(t(covterm), varterm))
    
    # update vkp
    if (i < p) {vkp = c(modar.back$coefficients[(i+2)], vkp)}
    if (i >= p) {vkp = c(0, vkp)}
}
predsd = sqrt(diag(Gamhat))
predsd = rev(predsd) # reverse order because the prediction goes backwards
predsd
```

In the plot below, predictions start on the left side of the red line (not including the red line):
```{r}
yupper = y.full + c(2*predsd, rep(0, n))
ylower = y.full - c(2*predsd, rep(0, n))
end = 100 # we only plot observations before this time
df = data.frame(
  t = (-35):end,
  y = y.full[1:(36+end)],
  yupper = yupper[1:(36+end)],
  ylower = ylower[1:(36+end)]
)

ggplot(df) + 
  geom_line(aes(x=t, y=y)) +
  geom_vline(xintercept = 1, color='red') + 
  geom_ribbon(aes(x=t, ymin=ylower, ymax=yupper), alpha=.2)
```
The predictions make sense: the prediction graph is almost linear, and the slope looks similar to the slope of the data. Also, the uncertainty interval is wider for predictions that are further away from the data.

\newpage
# 3
```{r}
y = pop$POPTHM
n = length(y)
```

## a
Fit the AR(3) model to the data.

```{r}
# fits y to AR(p) model
fit.ar = function(y, p) {
  n = length(y)
  Xmat = matrix(1, n-p, p+1)
  for(j in 1:p)
  {
     Xmat[, (j+1)] = y[(p+1-j):(n-j)]
  }
  linmod = lm(y[(p+1):n] ~ -1 + Xmat)
  return( linmod )
}

# I first used arima(y, order=c(3,0,0)), but it seems to give wrong coefficients?
```

The following are point estimates of phi0, phi1, phi2, phi3:
```{r}
modar = fit.ar(y, 3)
modar$coefficients
```
The following is a point estimate for sigma:
```{r}
sqrt((sum((modar$residuals)^2))/(n-3*2-1))
```


\newpage
## b
For better interpretability, I want to fit a model to the twice differenced series. Compare this twice-difference model to the AR(3) model fitted in the previous part. Are they similar?

```{r}
diff = rep(0, n) # twice difference of the time series
for (i in 3:n) {
  diff[i] = y[i] - 2*y[i-1] + y[i-2]
}
d = diff[3:n]

modar.d = fit.ar(d, 1)
alpha0 = modar.d$coefficients[1] %>% as.numeric()
alpha1 = modar.d$coefficients[2] %>% as.numeric()
alpha0
alpha1
```
The following four numbers are the coefficients obtained from rewriting the twice-diffrence model back in terms of $Y_t$:
```{r}
alpha0
alpha1 + 2 # close to phi1
-1-2*alpha1 # close to phi2
alpha1 # close to phi3
```
Except that alpha0 is not close to phi0, the other coefficients are similar in both models. It makes sense that alpha0 is not close to phi0 because the twice-difference operation kills any linear trend in the time series. 

## c
Compare the predictions of the two models for the next 60 time points. Are the
predictions similar? (4 points).

```{r}
k = 60

# compute AR(3) predictions
y.full = c(y, rep(-9999, k))
for (t in (n+1):(n+k)) {
  phis = modar$coefficients %>% as.numeric()
  #y.full[t] = sum(as.numeric(ar3$coef) * c(y.full[t-1], y.full[t-2], y.full[t-3], 1))
  y.full[t] = sum(phis * c(1, y.full[t-1], y.full[t-2], y.full[t-3]))
}
```


```{r, include=F}
linmod = lm(y ~ 1 + c(1:n))
a = linmod$coefficients[2] # slope
b = linmod$coefficients[1] # intercept
```

Below are predictions for the twice-difference $D_t$: 
```{r}
diff.full = c(diff, rep(-9999, k))
for (t in (n+1):(n+k)) {
  alphas = modar.d$coefficients %>% as.numeric()
  diff.full[t] = sum(alphas * c(1, diff.full[t-1]))
}
plot(1:(n+k), diff.full, type='l')
abline(v=n, col='red')
```
Then, $Y_t$ can be recovered from $D_t$ as follows:
```{r}
y.d = c(y, rep(-9999, k)) # d stands for difference
for (t in (n+1):(n+k)) {
  y.d[t] = diff.full[t] + 2*y.d[t-1] - y.d[t-2]
}
```

```{r}
start = 650
plotted.time = start:(n+k)
pred.time = (n+1):(n+k)
plot(plotted.time, y.full[plotted.time], type='l')
lines(pred.time, y.d[pred.time], col='red')
```
Both predictions are quite linear and increasing. But the predictions given by the twice-difference model are smaller and have a smaller slope. This happens because all the future predictions by the twice-difference model are determined by the last two entries of the data `y` and the twice-difference model stores no information about the linear trend of the data. Hence, the future slope is largely  determined by the slope given by the last two entries of the data. To take the linear trend of the whole data into account, one should consider the residuals of `y` obtained from linear regression in the first place, then do the same process as above to get predictions for the residuals, and then add back the linear part of the linear regression.

# 4
Download the FRED dataset on “Retail Sales: Beer, Wine, and Liquor Stores” from
https://fred.stlouisfed.org/series/MRTSSM4453USN. This is a monthly dataset
(the units are millions of dollars) and is not seasonally adjusted. Separate the last 48
observations from this dataset and keep them as a test dataset. Fit the AR(p) model
for each p = 1, 2, . . . , 24 for the training dataset and use it to predict the observations
in the test dataset. Evaluate the 24 models based on the accuracy of prediction and
report the model with the best prediction accuracy. (10 points).
```{r}
alc = read.csv('alc.csv')
colnames(alc)[2] = 'y'
```
```{r}
n.test = 48
n.train = nrow(alc) - n.test
y.train = alc$y[1:n.train]
y.test = alc$y[(n.train+1):nrow(alc)]
```

Let's choose p based on the mean square error of the predictions:
```{r}
mse = rep(-1, 24)
for (p in 1:24) {
  Xmat = matrix(1, n.train-p, p+1)
  for(j in 1:p) {
     Xmat[, (j+1)] = y.train[(p+1-j):(n.train-j)]
  }
  linmod = lm(y.train[(p+1):n.train] ~ -1 + Xmat)
  
  y.pred = c(y.train, rep(-9999, n.test))
  for (t in (n.train+1):nrow(alc)) {
    y.pred[t] = sum(c(1, y.pred[(t-1):(t-p)]) * linmod$coefficients)
  }
  
  y.pred = y.pred[(n.train+1):nrow(alc)]
  mse[p] = mean((y.pred - y.test)^2)
}
```

The first plot reveals a significant drop of MSE when p=12. The second plot shows that p=15 attains the lowest MSE, so we will choose p=15.
```{r}
plot(1:24, mse, type='l')
plot(12:24, mse[12:24], type='l')
```

Plot predictions vs. actual data to compare:
```{r}
p = 15
Xmat = matrix(1, n.train-p, p+1)
for(j in 1:p)
{
   Xmat[, (j+1)] = y.train[(p+1-j):(n.train-j)]
}
linmod = lm(y.train[(p+1):n.train] ~ -1 + Xmat)

y.pred = c(y.train, rep(-9999, n.test))
for (t in (n.train+1):nrow(alc)) {
  y.pred[t] = sum(c(1, y.pred[(t-1):(t-p)]) * linmod$coefficients)
}
pred.time = (n.train+1):nrow(alc)
y.pred = y.pred[pred.time]
plot(pred.time, y.test, type='l') # actual data
lines(pred.time, y.pred, col='red')
```


```{r, include=FALSE}
#p = 12
#ar = arima(y.train, order=c(p,0,0), method = 'ML') # why is this much slower than simply using linear regression? also, for large p, this gives errors.
#y.pred = c(y.train, rep(-9999, n.test))
#for (t in (n.train+1):nrow(alc)) {
#  y.pred[t] = sum(c(y.pred[(t-1):(t-p)], 1) * ar$coef)
#}
#plot(1:nrow(alc), y.pred, type='l', col='red')
#lines(1:nrow(alc), alc$y)
```











