---
title: "hw2code"
author: "Han-Yuan Hsu"
date: '2022-09-18'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
set.seed(111)
```
Reference: lecture code.

# 3
Download the Google Trends Data (for the United States) for the query mask. This
should be a monthly time series dataset that indicates the search popularity of this
query from January 2004 to September 2022. To this data, fit the single change point
model:
$$Y_t = \beta_0 + \beta_1 I\{t>c\} + Z_t,$$
where $Z_t$ are iid following $N(0, \sigma^2)$.

```{r}
mask <- read.csv('mask.csv', skip=1, header=T)
head(mask)
```
```{r}
n <- nrow(mask) # time series length, 225
t <- 1:n
y <- mask[,2] # values of the time series
```

## a
Provide a point estimate and 95% uncertainty interval for the changepoint parameter
c. Explain whether your answers make intuitive sense in the context of
this dataset (4 points).

For the prior of our model, $\beta_0, \beta_1, \log(\sigma), c$ are independent; $\beta_0, \beta_1, \log(\sigma)$ follow $\text{Unif}([-C, C])$, where C is large; and the changepoint $c$ is a discrete random variable and follows the uniform distribution on $\{3, 4, 5, ..., n-3\}$.

The following function, `log.post.c`, calculates the log of the posterior PMF of c (not normalized).
```{r}
log.post.c <- function(c) {
  X = matrix(1, nrow = n, ncol = 2)
  X[,2] = (t > c)
  #bhat = solve(t(X) %*% X) %*% t(X) %*% y
  mod = lm(y ~ 1 + X[,2])
  # log of posterior pdf of c
  log.post = (ncol(X) - n)/2*log(sum(mod$residuals^2)) - 0.5*log(det(t(X) %*% X))
  log.post
}  
```

Below, `values` is the vector of the posterior distribution of c, again not normalized:
```{r}
c.vals <- c(3:(n-3))
log.values <- as.numeric(lapply(c.vals, FUN=log.post.c))
log.values <- log.values - max(log.values)
values <- exp(log.values)
```

Below, `c.est` is the point estimate of c (maximum a posteriori estimator):
```{r}
ind.max = which.max(log.values)
c.est <- c.vals[ind.max]
print(c.est)
```
We can plot where `c.est` is (the red line):
```{r}
plot(t/12 + 2004, y, type='l')
abline(v=c.est/12 + 2004, col='red')
```
The estimated changepoint is located right before the spike when Covid broke out and drastically increased the number of searches for "mask", so the estimated changepoint does make sense. 

The code below calculates a 95% confidence interval centered at `c.est`:
```{r}
total = sum(values)
d = 1
conf.total = values[c.est]
while(conf.total <= .95 * total) {
  conf.total <- conf.total + values[c.est-d] + values[c.est+d]
  d <- d + 1
}
c(c.est-d, c.est+d)
```

## b
Provide point estimates and 95% marginal uncertainty intervals for the prechangepoint
mean level $\mu_0 := \beta_0$ and the post-changepoint mean level $\mu_1 := \beta_0 + \beta_1$ (4 points).

Let's first normalize `values` so that below, `post.pmf` is really the vector of the posterior distribution of c:
```{r}
post.pmf = values / sum(values)
#plot(c.vals, post.pmf, type='l')
```

We will create N=2000 iid samples of the posterior $(\vec\beta, c)$. We achieve that by the posterior distribution of c and the distribution of $\vec\beta$ conditioned on the data $\vec y$ and $c$. We already know the posterior distribution of c from part a. We can also derive the distribution of $\vec\beta \, |\, \vec y, c$ as follows: the model becomes linear in $\vec\beta$ when $c$ is fixed, and moreover, the prior distribution of $\vec\beta, \log\sigma$ conditioned on $c$ is still uniform because $c$ is assumed to be independent of $\vec\beta, \log\sigma$. Hence we can directly apply the results for linear models we learned in previous lectures and conclude that $\vec\beta \, |\, \vec y, c$ follows the multivariate t distribution
$$t_{n-2}\big(\hat\beta, \; \hat\sigma^2(X'X)^{-1}\big).$$
Note the parameters $\hat\beta$ and $\hat\sigma^2(X'X)^{-1}\big$ now depend on $c$. 

Thus, if a random variable C follows the distribution of the posterior c and a random vector $\vec B$ follows the multivariate t distribution above (which depends on C), then the joint, $(\vec B, C)$, will follow the distribution of the desired posterior $(\vec\beta, c)$ by the product rule of probability distributions.

The following code implements these ideas. `post.samples` is a matrix of the samples of $(c, \mu_0, \mu_1)$, where $\mu_0=\beta_0$ and $\mu_1=\beta_0+\beta_1$.

```{r}
library(mvtnorm)
N = 2000 #number of posterior samples
post.samples = matrix(-1, N, 3)
post.samples[,1] = sample(c.vals, N, replace = T, prob = post.pmf) # iid samples of posterior c
for(i in 1:N)
{
    cp = post.samples[i,1] # changepoint
    
    # below, we sample one mu_0 and one mu_1 from the appropriate t distribution.
    # mu_samples is c(mu_0, mu_1)
    X = matrix(1, nrow = n, ncol = 2)
    X[,2] = (t > cp)
    lin.model = lm(y ~ 1 + X[,2])
    bhat = lin.model$coefficients
    sighat = sqrt((sum((lin.model$residuals)^2))/(n-2)) #this is also denoted by the Residual Standard Error
    Sigma.mat = (sighat^2)*solve(t(X) %*% X)
    chiran = (rchisq(1, df = n-2))
    mu.samples = bhat + (rmvnorm(1, sigma = Sigma.mat))/(sqrt(chiran/(n-2)))
    mu.samples[2] = mu.samples[1] + mu.samples[2]
    
    post.samples[i,2:3] = mu.samples
}
```


```{r}
summary(post.samples[, 2])
summary(post.samples[, 3])
var(post.samples[, 2])
```

CI??????????????????????????????????????

```{r}
1/sqrt(.288*.05*225) # for chebyshev inequality CI
```
```{r}
quantile(post.samples[, 2], c(.025, .975))
```


## c
Comment on whether (2) is a good model for this dataset. (2 points)

The changepoint model is not a good. After the huge spike in 2020, the values don't stay near a hypothetical mean but rather decrease. In other words, the model assumes the queries for "mask" will stay high forever since 2020, but in reality, we know that the spike was caused by Covid and the epidemic is getting alleviated, which will bring the interest for masks down. A skewed Gaussian model with the mode being an unknown parameter is a better model. 

# 4
Download the Google Trends Data (for the United States) for the query golf. This
should be a monthly time series dataset that indicates the search popularity of this
query from January 2004 to September 2022. To this data, fit the model:
$$Y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 \cos(2\pi ft) + \beta_4 \sin(2\pi ft) + Z_t$$
with $Z_t$ iid following $N(0, \sigma^2)$.

```{r}
golf <- read.csv('golf.csv', skip=1, header=T)
head(golf)
y = golf[,2]
n = length(y)
t = 1:n
```
```{r}
plot(t, y, type='l')

n_half = as.integer(n/2)
pgram = abs(fft(y)[2:50])^2 / n # peaks at 19
plot(pgram, type='h')
225/19 # roughly 12. As expected, we have yearly trend
```

## a
Provide a point estimate and a 95% uncertainty interval for the unknown frequency
parameter f. (4 points)

Guess f near 1/12

```{r}
log.post.f = function(f) {
  X = matrix(1, nrow=n, ncol=5)
  X[,2] = t
  X[,3] = t^2
  X[,4] = cos(2*pi*f*t)
  X[,5] = sin(2*pi*f*t)
  
  lin.mod = lm(y ~ 1 + X[,2] + X[,3] + X[,4] + X[,5])
  
  -log(det(t(X) %*% X))/2 - log(sum((lin.mod$residuals)^2))*(n-5)/2
}
```


```{r}
x = seq(10, 14, by=.01)
plot(1/x, as.numeric(lapply(1/x, FUN=log.post.f)), type='l')
```
```{r}
opt = optimize(log.post.f, interval=c(.080, .085), maximum = T)
f.hat = opt$maximum
log.post.f.max = opt$objective
post.f.0 = function(f) { # posterior f but not normalized
  exp(log.post.f(f) - log.post.f.max)
}

res = .00001 # resolution
my.grid = seq(f.hat-.005, f.hat+.005, by=res)
total.integral = 0
for (x in my.grid) {
  total.integral = total.integral + res*post.f(x)
}

post.f = function(f) {
  post.f.0(f) / total.integral
}
```

```{r}
1/f.hat # very close to 12
```
```{r}
plot(1/x, as.numeric(lapply(1/x, FUN=post.f)), type='l')
```
```{r}
post.f(1/12) # 1/12 is even better than f.hat, kind of weird
post.f(f.hat)
```


How to get 95% CI?
```{r}
ci.prob = function(d) {
  
}
```

```{r}
# gridding 
# consider fixed sequence of values
# 1/12 +- 0.0001*2^n
# see the pdfs each term of the sequence maps

```

Redefine `post.f` to its normalized version:
```{r}
post.f = function(x) {
  post.f(x) / total.integral # error ... 
}
```


```{r}
d = res
ci.integral = res*post.f(f.hat)
while (ci.integral) {
  ci.integral = ci.integral + res*post.f(f.hat-d) + res*post.f(f.hat+d)
  d = d + res
}
```




```{r}
integrate(log.post.f, lower=f.hat-.001, f.hat+.001)
```

```{r}
integrate(function(x){x^2}, lower=0, upper=1)
```






## b
On a scatter plot of the data, plot your best estimate of the fitted function:
$$t \mapsto \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 \cos(2\pi ft) + \beta_4 \sin(2\pi ft)$$
along with appropriate uncertainty quantification. (5 points).

How should I get beta's?

```{r}
fitted.func = function(t) {
  
}

plot(t, y, type='l')
lines(t, )
```



c) Comment on whether model (3) is appropriate for this dataset. (2 points).

# 5
Download the FRED dataset on Total Construction Spending in the United States
from https://fred.stlouisfed.org/series/TTLCONS. This gives monthly seasonally
adjusted data on total construction spending in the United States in millions of
dollars from January 1993 to July 2022. To this dataset, fit the model:
$$Y_t = \beta_0 + \beta_1 t + \beta_2 (t-s_1)_+ + \beta_3 (t-s_2)_+ + Z_t$$
with $Z_t$ i.i.d âˆ¼ $N(0, \sigma^2)$. 

```{r}
df = read.csv('TTLCONS.csv')
y = df$TTLCONS
n = length(y)
```
```{r}
plot(1:n, y, type='l')
```


## a
Provide point estimates and 95% uncertainty intervals for the change of slope
parameters $s_1$ and $s_2$. (5 points)

For the prior of our model, $\beta_0, \beta_1, \log(\sigma)$ and the random vector $\vec s = (s_1, s_2)$ are independent; $\beta_0, \beta_1, \log(\sigma)$ follow $\text{Unif}([-C, C])$, where C is large; and $\vec s$ is a discrete random vector following the uniform distribution on
$$\{ (s_1, s_2) \in \mathbb{N}^2: \; 3 \leq s_1 < s_2 \leq n-3\}.$$

The following function, `logpost`, calculates the log of the posterior PMF of $\vec s$ (not normalized).
```{r}
#cps: change of slope points s_1, s_2.
logpost = function(cps) 
{
    cps = sort(cps)
    k = length(cps)
    X = matrix(1, nrow = n, ncol = (k+2))
    X[,2] = 1:n
    for(j in 1:k)
    {
        X[,(j+2)] = pmax(c(1:n) - cps[j], 0)
    }
    mod = lm(y ~ -1 + X) # -1 is needed because the default provides an intercept term, but
    # X already contains the intercept term
    log.value = ((ncol(X) - n)/2)*(log(sum(mod$residuals^2))) - (0.5*(log(det(t(X) %*% X))))
    return(log.value)
}
```


Below, `logpost.max` is the max of `logpost`, `s.hat` is where the max of `logpost` (hence the MAP estimator for $\vec s$).
```{r}
logpost.matrix = matrix(0, nrow=n-3, ncol=n-3) # saves logpost(c(s1, s2)) so that we don't need
# to compute again

logpost.max = logpost(c(3,4))
s.hat = c(0, 0) # MAP estimator for s
# the nested for loops take 20 seconds
for (s1 in 3:(n-3)) {
  if (s1+1 > n-3) {
    # then no need to go to the second for loop
    # need this if statement, or (s1+1):(n-3) will cause bug when s1+1 > n-3
    break
  }
  for (s2 in (s1+1):(n-3)) {
    m = logpost(c(s1, s2))
    logpost.matrix[s1, s2] = m
    if (m > logpost.max) {
      logpost.max = m
      s.hat[1] = s1
      s.hat[2] = s2
    }
  }
}
```

`s.hat`:
```{r}
s.hat
```


Below, `post.s` is the distribution of $\vec s$ (normalized), `post.s1` is the posterior distribution of $s_1$ (normalized), and `post.s1` is the posterior distribution of $s_2$ (normalized).
```{r}
post.s = matrix(0, nrow=n-3, ncol=n-3)
for (s1 in 3:(n-3)) {
  if (s1+1 > n-3) {
    # then no need to go to the second for loop
    # need this if statement, or (s1+1):(n-3) will cause bug when s1+1 > n-3
    break
  }
  for (s2 in (s1+1):(n-3)) {
    post.s[s1, s2] = exp(logpost.matrix[s1, s2] - logpost.max)
  }
}
# normalize
post.s = post.s / sum(post.s)
```

```{r}
post.s1 = apply(post.s, MARGIN=1, sum)
post.s2 = apply(post.s, MARGIN=2, sum)
# normalize
post.s1 = post.s1 / sum(post.s1)
post.s2 = post.s2 / sum(post.s2)
```

95% CI for $s_1$:
```{r}
d = 1
conf.total = post.s1[s.hat[1]]
while(conf.total <= .95) {
  conf.total <- conf.total + post.s1[s.hat[1]-d] + post.s1[s.hat[1]+d]
  d <- d + 1
}
c(s.hat[1]-d, s.hat[1]+d)
```
95% CI for $s_2$:
```{r}
d = 1
conf.total = post.s2[s.hat[2]]
while(conf.total <= .95) {
  conf.total <- conf.total + post.s2[s.hat[2]-d] + post.s2[s.hat[2]+d]
  d <- d + 1
}
c(s.hat[2]-d, s.hat[2]+d)
```

```{r, include=FALSE}
plot(1:n, y, type='l')
abline(v=s.hat[1], col='red')
abline(v=s.hat[2], col='red')
```


## b
On a scatter plot of the data, plot your best estimate of the fitted function along with appropriate uncertainty quantification. (5 points).

The following code generates `N` samples of $\vec s$ and $\vec\beta$ from the joint posterior distribution $\vec s, \vec\beta \;|\; \vec y$. The reasoning is similar to 3b.
```{r}
N = 2000 #number of posterior samples
# in post.samples, column 1, 2 is for samples from the distribution (s_1, s_2) | y
# column 3, 4, 5, 6 is for samples from (beta_0, ..., beta_3) | s_1, s_2, y
#
# equivalently, column 1 is for samples from s_1 | y
# and column 2 is for samples from s_2 | s_1, y
post.samples = matrix(-1, nrow=N, ncol=6)
post.samples[,1] = sample(3:(n-3), N, replace = T, prob = post.s1[3:(n-3)])

# make column 2
# s2.cond(s1) returns one sample from s_2 | s_1, y
s2.cond = function(s1) {
  s2.range = (s1+1):(n-3)
  cond.prob = post.s[s1, s2.range] / post.s1[s1] # conditional probability distribution represented by vector
  sample(s2.range, size=1, prob=cond.prob)
}
post.samples[,2] = sapply(post.samples[,1], FUN=s2.cond)

# make columns 3, 4, 5, 6
# column 3, 4, 5, 6 is for samples from (beta_0, ..., beta_3) | s_1, s_2, y, which is multivariate t
for(i in 1:N)
{
    p = 4
    
    s1 = post.samples[i,1]
    s2 = post.samples[i,2]
    X = matrix(1, nrow = n, ncol = p)
    X[,2] = 1:n
    X[,3] = pmax(c(1:n) - s1, 0)
    X[,4] = pmax(c(1:n) - s2, 0)
    lin.model = lm(y ~ -1 + X)
    bhat = lin.model$coefficients
    sighat = sqrt((sum((lin.model$residuals)^2))/(n-p))
    Sigma.mat = (sighat^2)*solve(t(X) %*% X)
    chiran = rchisq(1, df = n-p)
    beta.samples = bhat + (rmvnorm(1, sigma = Sigma.mat))/(sqrt(chiran/(n-p)))
    post.samples[i, 3:6] = beta.samples
}

```

Our best estimates for posterior $\vec s$ and $\vec\beta$ are `s.hat` and the mean of the distribution $\vec\beta \;|\; \vec s, y$, which is
$$\hat\beta = (X^\textsf{T}X)^{-1}X^\textsf{T}\vec y,$$
where $X = X(\hat s)$ depends on `s.hat`.
```{r}
# get beta hat
X = matrix(1, nrow = n, ncol = p)
X[,2] = 1:n
X[,3] = pmax(c(1:n) - s.hat[1], 0)
X[,4] = pmax(c(1:n) - s.hat[2], 0)
lin.model = lm(y ~ -1 + X)
best.bhat = lin.model$coefficients
best.bhat
```

In the following plot, the black graph is the actual values of the time series, and the red graph is
the fitted function.
```{r}
plot(1:n, y, type='l', xlab='t')
lines(lin.model$fitted.values, col='red')
```
We use the beta samples in `post.samples` to estimate 95% confidence intervals for posterior $\beta_0, \beta_1, \beta_2$, and $\beta_3$.

95% CI for $\beta_0$:
```{r}
quantile(post.samples[,3], c(.025, .975))
```

95% CI for $\beta_1$:
```{r}
quantile(post.samples[,4], c(.025, .975))
```

95% CI for $\beta_2$:
```{r}
quantile(post.samples[,5], c(.025, .975))
```

95% CI for $\beta_3$:
```{r}
quantile(post.samples[,6], c(.025, .975))
```

## c
Comment on whether the model is appropriate for this dataset. (2 points).
Visually, the model fits the data well. But in terms of forecasting, this model is bad; its future predictions will all be linear, while in reality, another change-of-slope point may occur. 

```{r, include=F}
plot(lin.model$residuals, type='l')
acf(lin.model$residuals, lag.max=300)
```



