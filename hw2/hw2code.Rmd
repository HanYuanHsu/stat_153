---
title: "hw2code"
author: "Han-Yuan Hsu"
date: '2022-09-18'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
```{r}
set.seed(111)
```
Reference: lecture code.

# 3
Download the Google Trends Data (for the United States) for the query mask. This
should be a monthly time series dataset that indicates the search popularity of this
query from January 2004 to September 2022. To this data, fit the single change point
model:
$$Y_t = \beta_0 + \beta_1 I\{t>c\} + Z_t,$$
where $Z_t$ are iid following $N(0, \sigma^2)$.

```{r}
mask <- read.csv('mask.csv', skip=1, header=T)
head(mask)
```
```{r}
n <- nrow(mask) # time series length, 225
t <- 1:n
y <- mask[,2] # values of the time series
```

## a
Provide a point estimate and 95% uncertainty interval for the changepoint parameter
c. Explain whether your answers make intuitive sense in the context of
this dataset (4 points).

For the prior of our model, $\beta_0, \beta_1, \log(\sigma), c$ are independent; $\beta_0, \beta_1, \log(\sigma)$ follow $\text{Unif}([-C, C])$, where C is large; and the changepoint $c$ is a discrete random variable and follows the uniform distribution on $\{3, 4, 5, ..., n-3\}$.

The following function, `log.post.c`, calculates the log of the posterior PMF of c (not normalized).
```{r}
log.post.c <- function(c) {
  X = matrix(1, nrow = n, ncol = 2)
  X[,2] = (t > c)
  #bhat = solve(t(X) %*% X) %*% t(X) %*% y
  mod = lm(y ~ 1 + X[,2])
  # log of posterior pdf of c
  log.post = (ncol(X) - n)/2*log(sum(mod$residuals^2)) - 0.5*log(det(t(X) %*% X))
  log.post
}  
```

Below, `values` is the vector of the posterior distribution of c, again not normalized:
```{r}
c.vals <- c(3:(n-3))
log.values <- as.numeric(lapply(c.vals, FUN=log.post.c))
log.values <- log.values - max(log.values)
values <- exp(log.values)
```

Below, `c.est` is the point estimate of c (maximum a posteriori estimator):
```{r}
ind.max = which.max(log.values)
c.est <- c.vals[ind.max]
print(c.est)
```
We can plot where `c.est` is (the red line):
```{r}
plot(t/12 + 2004, y, type='l')
abline(v=c.est/12 + 2004, col='red')
```
The estimated changepoint is located right before the spike when Covid broke out and drastically increased the number of searches for "mask", so the estimated changepoint does make sense. 

The code below calculates a 95% confidence interval centered at `c.est`:
```{r}
total = sum(values)
d = 1
conf.total = values[c.est]
while(conf.total <= .95 * total) {
  conf.total <- conf.total + values[c.est-d] + values[c.est+d]
  d <- d + 1
}
c(c.est-d, c.est+d)
```

## b
Provide point estimates and 95% marginal uncertainty intervals for the prechangepoint
mean level $\mu_0 := \beta_0$ and the post-changepoint mean level $\mu_1 := \beta_0 + \beta_1$ (4 points).

Let's first normalize `values` so that below, `post.pmf` is really the vector of the posterior distribution of c:
```{r}
post.pmf = values / sum(values)
#plot(c.vals, post.pmf, type='l')
```

We will create N=2000 iid samples of the posterior $(\vec\beta, c)$. We achieve that by the posterior distribution of c and the distribution of $\vec\beta$ conditioned on the data $\vec y$ and $c$. We already know the posterior distribution of c from part a. We can also derive the distribution of $\vec\beta \, |\, \vec y, c$ as follows: the model becomes linear in $\vec\beta$ when $c$ is fixed, and moreover, the prior distribution of $\vec\beta, \log\sigma$ conditioned on $c$ is still uniform because $c$ is assumed to be independent of $\vec\beta, \log\sigma$. Hence we can directly apply the results for linear models we learned in previous lectures and conclude that $\vec\beta \, |\, \vec y, c$ follows the multivariate t distribution
$$t_{n-2}\big(\hat\beta, \; \hat\sigma^2(X'X)^{-1}\big).$$
Note the parameters $\hat\beta$ and $\hat\sigma^2(X'X)^{-1}\big$ now depend on $c$. 

Thus, if a random variable C follows the distribution of the posterior c and a random vector $\vec B$ follows the multivariate t distribution above (which depends on C), then the joint, $(\vec B, C)$, will follow the distribution of the desired posterior $(\vec\beta, c)$ by the product rule of probability distributions.

The following code implements these ideas. `post.samples` is a matrix of the samples of $(c, \mu_0, \mu_1)$, where $\mu_0=\beta_0$ and $\mu_1=\beta_0+\beta_1$.

```{r}
library(mvtnorm)
N = 2000 #number of posterior samples
post.samples = matrix(-1, N, 3)
post.samples[,1] = sample(c.vals, N, replace = T, prob = post.pmf) # iid samples of posterior c
for(i in 1:N)
{
    cp = post.samples[i,1] # changepoint
    
    # below, we sample one mu_0 and one mu_1 from the appropriate t distribution.
    # mu_samples is c(mu_0, mu_1)
    X = matrix(1, nrow = n, ncol = 2)
    X[,2] = (t > cp)
    lin.model = lm(y ~ 1 + X[,2])
    bhat = lin.model$coefficients
    sighat = sqrt((sum((lin.model$residuals)^2))/(n-2)) #this is also denoted by the Residual Standard Error
    Sigma.mat = (sighat^2)*solve(t(X) %*% X)
    chiran = (rchisq(1, df = n-2))
    mu.samples = bhat + (rmvnorm(1, sigma = Sigma.mat))/(sqrt(chiran/(n-2)))
    mu.samples[2] = mu.samples[1] + mu.samples[2]
    
    post.samples[i,2:3] = mu.samples
}
```


```{r}
summary(post.samples[, 2])
summary(post.samples[, 3])
var(post.samples[, 2])
```

CI??????????????????????????????????????

```{r}
1/sqrt(.288*.05*225) # for chebyshev inequality CI
```
```{r}
quantile(post.samples[, 2], c(.025, .975))
```


## c
Comment on whether (2) is a good model for this dataset. (2 points)

The changepoint model is not a good. After the huge spike in 2020, the values don't stay near a hypothetical mean but rather decrease. In other words, the model assumes the queries for "mask" will stay high forever since 2020, but in reality, we know that the spike was caused by Covid and the epidemic is getting alleviated, which will bring the interest for masks down. A skewed Gaussian model with the mode being an unknown parameter is a better model. 

# 4
Download the Google Trends Data (for the United States) for the query golf. This
should be a monthly time series dataset that indicates the search popularity of this
query from January 2004 to September 2022. To this data, fit the model:
$$Y_t = \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 \cos(2\pi ft) + \beta_4 \sin(2\pi ft) + Z_t$$
with $Z_t$ iid following $N(0, \sigma^2)$.

```{r}
golf <- read.csv('golf.csv', skip=1, header=T)
head(golf)
y = golf[,2]
n = length(y)
t = 1:n
```
```{r}
plot(t, y, type='l')

n_half = as.integer(n/2)
pgram = abs(fft(y)[2:50])^2 / n # peaks at 19
plot(pgram, type='h')
225/19 # roughly 12. As expected, we have yearly trend
```


a) Provide a point estimate and a 95% uncertainty interval for the unknown frequency
parameter f. (4 points)

Guess f near 1/12

```{r}
log.post.f = function(f) {
  X = matrix(1, nrow=n, ncol=5)
  X[,2] = t
  X[,3] = t^2
  X[,4] = cos(2*pi*f*t)
  X[,5] = sin(2*pi*f*t)
  
  lin.mod = lm(y ~ 1 + X[,2] + X[,3] + X[,4] + X[,5])
  
  -log(det(t(X) %*% X))/2 - log(sum((lin.mod$residuals)^2))*(n-5)/2
}
```


```{r}
x = seq(10, 14, by=.01)
plot(1/x, as.numeric(lapply(1/x, FUN=log.post.f)), type='l')
```
```{r}
opt = optimize(log.post.f, interval=c(.080, .085), maximum = T)
f.hat = opt$maximum
log.post.f.max = opt$objective
post.f = function(f) {
  exp(log.post.f(f) - log.post.f.max)
}
```
```{r}
1/f.hat # very close to 12
```
```{r}
plot(1/x, as.numeric(lapply(1/x, FUN=post.f)), type='l')
```
```{r}
post.f(1/12) # 1/12 is even better than f.hat, kind of weird
post.f(f.hat)
```


How to get 95% CI?
```{r}
ci.prob = function(d) {
  
}
```




b) On a scatter plot of the data, plot your best estimate of the fitted function:
$$t \mapsto \beta_0 + \beta_1 t + \beta_2 t^2 + \beta_3 \cos(2\pi ft) + \beta_4 \sin(2\pi ft)$$
along with appropriate uncertainty quantification. (5 points).

How should I get beta's?

```{r}
fitted.func = function(t) {
  
}

plot(t, y, type='l')
lines(t, )
```



c) Comment on whether model (3) is appropriate for this dataset. (2 points).


