---
title: "hw3code"
author: "Han-Yuan Hsu"
date: '2022-10-04'
output: pdf_document
---
```{r}
library(dplyr)
set.seed(111)
```
Reference: lecture code

# 1
A noisy measurement device is being examined for understanding the distribution of
the errors that are being produced by it. Suppose that ten measurements led to the
following observations on the errors made by the device:
```{r}
err = c(-0.69, -4.26, 0.14, -0.86, 0.42, 24.21, 0.51, -1.23, 2.30, 4.15)
```

## a
```{r}
lik1 = function(sigma) {
  if (sigma==0) {return(0)} # the formula below is undefined for sigma=0, but the limit is 0
  
  1/(sqrt(2*pi)*sigma)^10 * exp(-sum(err^2)/2/sigma^2)
}

lik2 = function(sigma) {
  if (sigma==0) {return(0)}
  
  1/(2*sigma)^10 * exp(-sum(abs(err))/sigma)
}

lik3 = function(sigma) {
  prod = 1
  for (i in 1:10) {
    term = sigma / (err[i]^2 + sigma^2) / pi 
    prod = prod * term
  }
  prod
}
```


The range of the integrals is $(e^{-15}, e^{15})$, but $e^{15}$ is too large for numerical integration. The following shows integrating up to 100 does a good approximation:
```{r}
lik1(100)
lik2(100)
lik3(100)
```


```{r}
res = .001
my.grid = seq(exp(-15), 100+exp(-15), by=res)

lik1.vals = sapply(my.grid, FUN=lik1)
evid1 = 1/30 * sum(lik1.vals / my.grid) * res

lik2.vals = sapply(my.grid, FUN=lik2)
evid2 = 1/30 * sum(lik2.vals / my.grid) * res

lik3.vals = sapply(my.grid, FUN=lik3)
evid3 = 1/30 * sum(lik3.vals / my.grid) * res

```

The evidences:
```{r}
evid1
evid2
evid3
```


## b
Normalize the three evidences to obtain posterior probabilities for the three models.
Which model has the highest posterior probability? Comment on whether
your results seem intutively sensible. (2 points).

```{r}
normalizing.const = evid1 + evid2 + evid3
evid1 / normalizing.const
evid2 / normalizing.const
evid3 / normalizing.const
```
Model 3 has the highest posterior probability. That makes sense -- Cauchy distribution has the fattest tail compared with the distrbutions of the other two models that decay exponentially. The error data has an outlier 24.21, so a heavy-tail distribtuion makes this outlier more possible. 

\newpage
# 2
```{r}
dat = read.csv('HW3Data153Fall2022.csv')
head(dat)
n = length(dat$x)
```
```{r}
plot(dat$x, dat$y)
```



## a
Numerically calculate the Evidence for each of these models given the observed
data. Report the normalized evidences. Which model has the highest evidence and
what is the value of this highest evidence? (6 points)

```{r}
beta0.vals = seq(-10, 10, by=.1)
beta1.vals = seq(-10, 10, by=.1)
sig.vals = exp(seq(-10, 10, by=.1))
vals.length = length(beta0.vals)

loglik1 = function(param) {
  b0 = param[1]; b1 = param[2]; sig = param[3]
  n*(log(sig)-log(pi)) - sum(log((dat$y - b1*dat$x - b0)^2 + sig^2))
}

loglik2 = function(param) {
  b0 = param[1]; b1 = param[2]; sig = param[3]
  -n*log(2*sig) - sum(abs(dat$y - b1*dat$x - b0))/sig
}

loglik3 = function(param) {
  b0 = param[1]; sig = param[2]
  n*(log(sig)-log(pi)) - sum(log((dat$y - b0)^2 + sig^2))
}

loglik4 = function(param) {
  b0 = param[1]; sig = param[2]
  -n*log(2*sig) - sum(abs(dat$y - b0))/sig
}
```

expand grid and apply loglik1
subtract max
exp
normalize

```{r}
vals = expand.grid(beta0.vals, beta1.vals, sig.vals)
```
```{r}
#loglik1.vals = apply(vals, MARGIN=1, FUN=loglik1) # takes 3 minutes
#save(loglik1.vals, file='loglik1.vals.RData')
load(file='loglik1.vals.RData')
```
```{r}
loglik1.vals.max = max(loglik1.vals)
loglik1.vals = loglik1.vals - loglik1.vals.max
lik1.vals = exp(loglik1.vals) # the true likelihood values should be this times exp(loglik1.vals.max)
```

Below, the true evidence of model 1 is proportional to `evid1.scaled` times exp(`loglik1.vals.max`), where
the proportionality constant is the same for all four models. So, to choose the best model, we find the largest `evidm.scaled` times exp(`loglikm.vals.max`) for m=1,2,3,4.
```{r}
evid1.scaled = sum(lik1.vals) # I didn't multiply the prior here because it is the same for all 4 models
evid1.scaled
loglik1.vals.max
```

```{r}
#loglik2.vals = apply(vals, MARGIN=1, FUN=loglik2)
#save(loglik2.vals, file='loglik2.vals.RData')
load(file='loglik2.vals.RData')
```

```{r}
loglik2.vals.max = max(loglik2.vals)
loglik2.vals = loglik2.vals - loglik2.vals.max
lik2.vals = exp(loglik2.vals) # the true likelihood values should be this times exp(loglik2.vals.max)
```
```{r}
evid2.scaled = sum(lik2.vals)
evid2.scaled
loglik2.vals.max
```


```{r}
vals2 = expand.grid(beta0.vals, sig.vals)
```

```{r}
#loglik3.vals = apply(vals2, MARGIN=1, FUN=loglik3)
#save(loglik3.vals, file='loglik3.vals.RData')
#loglik4.vals = apply(vals2, MARGIN=1, FUN=loglik4)
#save(loglik4.vals, file='loglik4.vals.RData')
load('loglik3.vals.RData')
load('loglik4.vals.RData')
```

```{r}
loglik3.vals.max = max(loglik3.vals)
loglik3.vals = loglik3.vals - loglik3.vals.max
lik3.vals = exp(loglik3.vals) # the true likelihood values should be this times exp(loglik3.vals.max)
```
```{r}
evid3.scaled = sum(lik3.vals)
evid3.scaled
loglik3.vals.max
```
```{r}
loglik4.vals.max = max(loglik4.vals)
loglik4.vals = loglik4.vals - loglik4.vals.max
lik4.vals = exp(loglik4.vals) # the true likelihood values should be this times exp(loglik3.vals.max)
```
```{r}
evid4.scaled = sum(lik4.vals)
evid4.scaled
loglik4.vals.max
```

Calculate normalized evidences:
```{r}
evid1 = evid1.scaled
evid2 = evid2.scaled * exp(loglik2.vals.max - loglik1.vals.max)
evid3 = evid3.scaled * exp(loglik3.vals.max - loglik1.vals.max)
evid4 = evid4.scaled * exp(loglik4.vals.max - loglik1.vals.max)
normalizing.const = evid1 + evid2 + evid3 + evid4
evid1 = evid1 / normalizing.const
evid2 = evid2 / normalizing.const
evid3 = evid3 / normalizing.const
evid4 = evid4 / normalizing.const
```
```{r}
evid1 # should be almost 1
evid2
evid3
evid4
```
Model 1 has the highest evidence, with an evidence almost 1 because the evidences for other models are very small.

## b
```{r}
ind.max = which.max(loglik1.vals)
beta0.hat = vals[ind.max, 1]
beta1.hat = vals[ind.max, 2]
sig.hat = vals[ind.max, 3]
```
```{r}
beta0.hat
beta1.hat
sig.hat
```
Below, the red line is $\hat\beta_0 + \hat\beta_1 x$:
```{r}
plot(dat$x, dat$y)
abline(beta0.hat, beta1.hat, col='red')
```

To get uncertainty quantification for the parameters, we sample from the posterior distribution:
```{r}
N = 1000000
post.pdf = lik1.vals / sum(lik1.vals) # posterior pdf of beta0, beta1, sigma
ind.samples = sample(1:nrow(vals), N, replace=T, prob=post.pdf)
beta0.samples = vals[ind.samples, 1]
beta1.samples = vals[ind.samples, 2]
sig.samples = vals[ind.samples, 3]
```

Below are the probabilities of each parameter equaling its estimator:
```{r}
sum(beta0.samples == beta0.hat) / length(beta0.samples)
sum(beta1.samples == beta1.hat) / length(beta1.samples)
sum(sig.samples == sig.hat) / length(sig.samples)
```
Below is a 95% credibility interval for sigma:
```{r}
quantile(sig.samples, c(.025, .975))
```
\newpage
# 3
R has an inbuilt dataset called state which gives some data on 50 states in America
from the 1970s. You can access this dataset via (see help(state) for information
about the data)
We want to fit a linear model to this dataset with life expectancy as the response
variable and some subset of the remaining seven explanatory variables (including the
intercept term). Your goal is to figure out which subset of the explanatory variables
provides the best explanation for the response variable in a linear model.

```{r}
data(state); dt = data.frame(state.x77, row.names = state.abb)
head(dt)
```


## a
Use the Evidence-based Bayesian model selection method from Lecture 11 to
calculate the evidences for each of the 2^7 = 128 models (obtaining by taking all
possible subsets of the explanatory variables along with the intercept term). How
many of the 128 models get nontrivial Evidences (after normalization so that the
Evidences sum to one)? Describe the models getting high evidences? Do the
results of your analysis seem sensible? (6 points)

```{r}
#y = dt$Life.Exp
g <- lm(Life.Exp ~ ., data=dt)
summary(g)
```

`all01` below indexes subsets of the features in a 1/0 fashion.
```{r}
fullX = model.matrix(g)
numvar = 7
all01 = expand.grid(replicate(numvar, 0:1, simplify = FALSE))
all01 = cbind(rep(1, nrow(all01)), all01)
colnames(all01) = colnames(fullX)
logEvid = rep(-1, nrow(all01))
```
```{r}
head(all01)
```

The following calculates the log evidence for each model `mm` in `all01`:
```{r}
for(mm in 1:nrow(all01))
{
    inds = all01[mm,]
    Xmat = fullX[,(inds == 1)]
    if(mm == 1) {
      # this is the model with the intercept only, which is a trivial case
      # if we don't run this if statement, weird bugs happen
      Xmat = as.matrix(rep(1, nrow(dt)), nrow=nrow(dt), ncol=1)
    }
    p = ncol(Xmat)
    n = nrow(Xmat)
    md = lm(dt$Life.Exp ~ -1 + Xmat)
    # the following log evidence formula for model mm comes from lecture 11 notes.
    logEvid[mm] = (lgamma(p/2)) - ((p/2)*(log((sum(md$fitted.values^2))))) + (lgamma((n-p-1)/2)) - (((n-p)/2)*(log((sum(md$residuals^2))/2)))
}
```

The first 10 models with highest evidence:
```{r}
logEvid.sorted = sort(logEvid, decreasing = T, index.return=T)
r = 10 # show first r models
high.evid.ind = logEvid.sorted$ix[1:r]
all01[high.evid.ind, ]
```
```{r, include=F}
explain.var.names = colnames(all01) # names of explainatory variables

logEvid.sorted = sort(logEvid, decreasing = T, index.return=T)
r = 10 # show first r models
high.evid.ind = logEvid.sorted$ix[1:r]
for (i in 1:r) {
  features01 = all01[high.evid.ind[i], ]
  print(explain.var.names[as.logical(features01)])
}
```
All of these 10 models include the murder feature, which makes sense because in the summary of the model `g` that uses all the features, we see that only murder is very significant. Also, it turns out models with fewer features are preferred, which makes sense because the evidence formula we use here penalizes models with a large number of features. Indeed, the best model only has one feature, which is murder of course. The second-best model includes HS.Grad feature, which also makes sense because the feature has the second best significance level, as shown in the summary of `g`.

Get normalized evidences from log evidences:
```{r}
evid = logEvid - max(logEvid)
evid = exp(evid)
evid = evid / sum(evid)
```


## b and c
Calculate the best model (among the 128 possible models) using the BIC. Compare
this model with the models obtaining high evidence from part (a). (3 points)
Calculate the best model (among the 128 possible models) using the AIC. Compare
this model with the models obtaining high evidence from part (a). (3 points)

```{r}
bic = rep(-9999, nrow(all01))
aic = rep(-9999, nrow(all01))
for(mm in 1:nrow(all01))
{
    inds = all01[mm,]
    Xmat = fullX[,(inds == 1)]
    if(mm == 1) {
      # this is the model with the intercept only, which is a trivial case
      # if we don't run this if statement, weird bugs happen
      Xmat = as.matrix(rep(1, nrow(dt)), nrow=nrow(dt), ncol=1)
    }
    p = ncol(Xmat)+1 # number of parameters, betas AND sigma
    n = nrow(Xmat)
    md = lm(dt$Life.Exp ~ -1 + Xmat)
    sig.hat.squared = sum((md$residuals)^2) / n
    bic[mm] = n + n*log(2*pi*sig.hat.squared) + p*log(n)
    aic[mm] = n + n*log(2*pi*sig.hat.squared) + p*2
}
```


```{r}
bic.sorted = sort(bic, decreasing = T, index.return=T)
high.bic.ind = bic.sorted$ix[1:10]
all01[high.bic.ind, ]
```


Completely different from evidence-based model selection...

```{r}
aic.sorted = sort(aic, decreasing = T, index.return=T)
high.aic.ind = aic.sorted$ix[1:r]
all01[high.aic.ind, ]
```



```{r, include=F}
logEvid.sorted$x[bic.sorted$ix[1:10]]
```






```{r}
logEvid.ind = logEvid.sorted$ix[1:30]
plot(logEvid.ind, bic[logEvid.ind])

```


\newpage
# 4
Download the google trends time series dataset for the query yahoo. This should be
a monthly time series dataset that indicates the search popularity of this query from
January 2004 to August 2022. The goal of this exercise is to use model selection to
figure out the best polynomial trend model among the degrees k = 1, 2, 3, 4, 5, 6, 7, 8. To prevent numerical instability issues, take x_i to be some scaled version of time (for example, take x_i = i/n).
```{r}
yahoo <- read.csv('../hw1/yahoo.csv', header=T, skip=1)
colnames(yahoo) = c('Month', 'y')
yahoo <- yahoo[1:(nrow(yahoo)-1), ] # drop last row, which corresponds to Sep 2022
#yahoo.ts <- ts(yahoo$y, start = c(2004, 1), end = c(2022, 8), frequency = 12)
head(yahoo)
n = length(yahoo$y) # time series length
```
```{r}
x = c(1:n) / n # scaled time
y = yahoo$y
```


## a
Use the Evidence-based Bayesian model selection method from Lecture 11 to
calculate the evidences for each of the above 8 models. Report the normalized
evidences of each of the 8 models. Which model has the highese evidence? Does
this model selection method select models that seemings overfit? (6 points)
```{r}
num.models = 8
logEvid = rep(-1, num.models)

Xmat = matrix(rep(1, n), nrow=n)
for(mm in 1:num.models)
{
    Xmat = cbind(Xmat, x^mm)
    p = ncol(Xmat) # here, p is the number of betas only
    md = lm(y ~ -1 + Xmat)
    # the following log evidence formula for model mm comes from lecture 11 notes.
    logEvid[mm] = (lgamma(p/2)) - ((p/2)*(log((sum(md$fitted.values^2))))) + (lgamma((n-p-1)/2)) - (((n-p)/2)*(log((sum(md$residuals^2))/2)))
    
}
```

```{r}
which.max(logEvid)
```
7 is chosen, which is large!!! 


## b and c
Calculate the best model using the BIC and the AIC. Compare this model with the models
obtaining high evidence from part (a).

```{r}
bic = rep(-9999, num.models)
aic = rep(-9999, num.models)
Xmat = matrix(rep(1, n), nrow=n)
for(mm in 1:num.models)
{
    Xmat = cbind(Xmat, x^mm)
    md = lm(y ~ -1 + Xmat)
    p = ncol(Xmat)+1 # number of parameters, betas AND sigma
    sig.hat.squared = sum((md$residuals)^2) / n
    bic[mm] = n + n*log(2*pi*sig.hat.squared) + p*log(n)
    aic[mm] = n + n*log(2*pi*sig.hat.squared) + p*2
}
```

```{r}
which.min(bic)
which.min(aic)
```
Both are same as what evidence gave!!!

```{r}
plot(1:n, y, type='l')
```

\newpage
# 5
Download the FRED dataset on “Retail Sales: Beer, Wine, and Liquor Stores” from
https://fred.stlouisfed.org/series/MRTSSM4453USN. This is a monthly dataset
(the units are millions of dollars) and is not seasonally adjusted. To this data, we want
to fit one of the models $M_{kl}$ where k ranges in 0, 1, 2, 3, 4, 5 and l ranges in 0, 1, 2, 3, 4, 5.

```{r}
alc = read.csv('alc.csv')
y = alc[,2]
n = length(y)
```
```{r}
plot(1:n, y, type='l')
```



## a
Use the Evidence-based Bayesian model selection method from Lecture 11 to
calculate the evidences for each of the above 36 models. Report the normalized
evidences of each of the 36 models. Which models have high evidence? Does this
model selection method favor models that seemingly overfit? (6 points)

```{r}
k.vals = 0:5
l.vals = 0:5
logEvid = rep(-9999, 36)
ind = 1 # index for accessing elements of logEvid

Poly = matrix(rep(1, n), ncol=1) # polynomial part of the model matrix
for (k in k.vals) {
  Xmat = Poly
  for (l in l.vals) {
    p = ncol(Xmat)
    md = lm(y ~ -1 + Xmat)
    # the following log evidence formula for model mm comes from lecture 11 notes.
    logEvid[ind] = (lgamma(p/2)) - ((p/2)*(log((sum(md$fitted.values^2))))) + (lgamma((n-p-1)/2)) - (((n-p)/2)*(log((sum(md$residuals^2))/2)))
    
    ind = ind + 1
    Xmat = cbind(Xmat, cos(2*pi*(l+1)/12 * (1:n)))
    Xmat = cbind(Xmat, sin(2*pi*(l+1)/12 * (1:n)))
  }
  Poly = cbind(Poly, ((1:n)/n)^(k+1))
}
```

Below are the top 5 models with high evidence:
```{r}
logEvid.sorted = sort(logEvid, decreasing = T, index.return=T)
ind = logEvid.sorted$ix[1:5]
k.and.l = expand.grid(l.vals, k.vals)
colnames(k.and.l) = c('l', 'k')
k.and.l[ind,]
```
The best model, chosen by Bayesian evidence, has k=4 and l=5.

Calculate evidences:
```{r}
logEvid.max = max(logEvid)
logEvid = logEvid - logEvid.max
evid = exp(logEvid)
evid = evid / sum(evid)
plot(1:36, evid, type='h')
```
The normalized evidence of the models with the highest evidence:
```{r}
evid[ind[1]]
```
Does this
model selection method favor models that seemingly overfit?
????????????????????????????
?????????????????????????

## b and c
Calculate the best model using the BIC and AIC. Compare with the models
obtaining high evidence from part (a).

```{r}
k.vals = 0:5
l.vals = 0:5
bic = rep(-9999, 36)
aic = rep(-9999, 36)
ind = 1 # index for accessing elements of logEvid

Poly = matrix(rep(1, n), ncol=1) # polynomial part of the model matrix
for (k in k.vals) {
  Xmat = Poly
  for (l in l.vals) {
    p = ncol(Xmat)+1 # number of parameters, betas AND sigma
    md = lm(y ~ -1 + Xmat)
    sig.hat.squared = sum((md$residuals)^2) / n
    bic[ind] = n + n*log(2*pi*sig.hat.squared) + p*log(n)
    aic[ind] = n + n*log(2*pi*sig.hat.squared) + p*2
    
    ind = ind + 1
    Xmat = cbind(Xmat, cos(2*pi*(l+1)/12 * (1:n)))
    Xmat = cbind(Xmat, sin(2*pi*(l+1)/12 * (1:n)))
  }
  Poly = cbind(Poly, ((1:n)/n)^(k+1))
}
```

```{r}
plot(1:36, bic, type='l', xaxt='n', main='BIC')
axis(side=1, at=(1:6)*6)
```
```{r}
plot(1:36, aic, type='l', xaxt='n', main='AIC')
axis(side=1, at=(1:6)*6)
```
```{r}
k.and.l[which.min(bic), ] # best model chosen by bic
k.and.l[which.min(aic), ] # best model chosen by aic
```
We see that BIC chooses the same model as Bayesian evidence does, whereas AIC chooses the model with a larger l, i.e. l=5. This makes sense because BIC penalizes models with many parameters. From the plots of BIC and AIC above, we see that BIC and AIC both favor large l, just like Bayesian Evidence.

\newpage
# 6
A classic time series dataset is the Box and Jenkins airline passenger data (can be
accessed in R via data(AirPassengers)). This gives monthly totals of international
airline passengers from 1949 to 1960. There are n = 144 observations in total corresponding
to 12 years.










